// app/api/voice-dictation-transcribe/route.ts
// NOUVELLE API: Transcription + Extraction uniquement
// Le frontend affichera ensuite DiagnosisForm et ProfessionalReport

import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';

// Initialize OpenAI client
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export const runtime = 'nodejs';
export const maxDuration = 180; // 3 minutes max

// ============================================
// FUNCTION 1: TRANSCRIBE AUDIO
// ============================================
async function transcribeAudio(audioFile: File): Promise<{
  text: string;
  duration: number;
  language: string;
}> {
  console.log('üîä Step 1: Starting audio transcription...');
  console.log(`   Audio file: ${audioFile.name} (${audioFile.size} bytes)`);

  try {
    const transcription = await openai.audio.transcriptions.create({
      file: audioFile,
      model: 'whisper-1',
      language: 'fr', // French by default, Whisper will auto-detect if needed
      response_format: 'verbose_json',
    });

    console.log('‚úÖ Transcription completed');
    console.log(`   Text length: ${transcription.text.length} characters`);
    console.log(`   Duration: ${transcription.duration} seconds`);
    console.log(`   Language: ${transcription.language}`);

    return {
      text: transcription.text,
      duration: transcription.duration || 0,
      language: transcription.language || 'fr',
    };
  } catch (error: any) {
    console.error('‚ùå Transcription failed:', error.message);
    throw new Error(`Transcription failed: ${error.message}`);
  }
}

// ============================================
// FUNCTION 2: EXTRACT CLINICAL DATA
// ============================================
async function extractClinicalData(transcriptionText: string): Promise<{
  patientInfo: any;
  clinicalData: any;
  aiQuestions: any;
  referralInfo?: any;
  consultationType: 'standard' | 'specialist_referral';
}> {
  console.log('üìä Step 2: Extracting clinical data with GPT-4o...');

  const extractionPrompt = `Tu es un assistant m√©dical expert. Analyse cette transcription de consultation m√©dicale et extrais les informations suivantes au format JSON strict:

‚ö†Ô∏è IMPORTANT: PR√âSERVE TOUTES les hypoth√®ses diagnostiques, notes cliniques, et raisonnements du m√©decin. NE PAS SUPPRIMER ces informations cruciales.

{
  "patientInfo": {
    "firstName": "pr√©nom du patient",
    "lastName": "nom du patient",
    "age": nombre (age en ann√©es),
    "gender": "M" ou "F" ou "Other",
    "email": "email si mentionn√©",
    "phone": "t√©l√©phone si mentionn√©"
  },
  "clinicalData": {
    "chiefComplaint": "motif principal de consultation",
    "symptoms": ["sympt√¥me 1", "sympt√¥me 2"],
    "duration": "dur√©e des sympt√¥mes",
    "severity": "l√©g√®re/mod√©r√©e/s√©v√®re",
    "medicalHistory": ["ant√©c√©dent 1", "ant√©c√©dent 2"],
    "currentMedications": ["m√©dicament 1", "m√©dicament 2"],
    "allergies": ["allergie 1", "allergie 2"],
    "vitalSigns": {
      "temperature": "en ¬∞C si mentionn√©",
      "bloodPressure": "en mmHg si mentionn√©",
      "heartRate": "en bpm si mentionn√©",
      "respiratoryRate": "en /min si mentionn√©"
    }
  },
  "aiQuestions": {
    "primaryConcern": "pr√©occupation principale",
    "additionalSymptoms": ["autres sympt√¥mes"],
    "riskFactors": ["facteurs de risque identifi√©s"]
  },
  "doctorNotes": {
    "clinicalHypotheses": ["hypoth√®se 1 du m√©decin", "hypoth√®se 2"],
    "differentialDiagnoses": ["diagnostic diff√©rentiel 1", "diagnostic diff√©rentiel 2"],
    "clinicalReasoning": "raisonnement clinique du m√©decin",
    "treatmentPlan": "plan th√©rapeutique pr√©liminaire du m√©decin",
    "observations": "observations cliniques importantes du m√©decin",
    "recommendations": ["recommandation 1", "recommandation 2"]
  },
  "referralInfo": {
    "isReferral": true/false,
    "referringPhysician": "nom du m√©decin r√©f√©rent si c'est une r√©f√©rence",
    "specialty": "sp√©cialit√© si mentionn√©e",
    "reasonForReferral": "raison de la r√©f√©rence"
  }
}

üéØ R√àGLES CRITIQUES:
1. PR√âSERVE ABSOLUMENT toutes les hypoth√®ses diagnostiques du m√©decin dans "doctorNotes.clinicalHypotheses"
2. CONSERVE tous les diagnostics diff√©rentiels mentionn√©s par le m√©decin
3. GARDE le raisonnement clinique original du m√©decin
4. NE TRANSFORME PAS ni ne SUPPRIME les pens√©es cliniques du m√©decin
5. Si le m√©decin mentionne "je pense que", "probablement", "possiblement" ‚Üí PR√âSERVE dans doctorNotes

Transcription:
${transcriptionText}

R√©ponds UNIQUEMENT avec le JSON, sans texte additionnel.`;

  try {
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'Tu es un assistant m√©dical expert qui extrait des donn√©es cliniques structur√©es.',
        },
        {
          role: 'user',
          content: extractionPrompt,
        },
      ],
      temperature: 0.3,
      response_format: { type: 'json_object' },
    });

    const extractedData = JSON.parse(completion.choices[0].message.content || '{}');

    console.log('‚úÖ Extraction completed');
    console.log(`   Patient: ${extractedData.patientInfo?.firstName} ${extractedData.patientInfo?.lastName}`);
    console.log(`   Chief complaint: ${extractedData.clinicalData?.chiefComplaint}`);
    
    // Log doctor's clinical hypotheses if present
    if (extractedData.doctorNotes?.clinicalHypotheses?.length > 0) {
      console.log(`   ‚öïÔ∏è Doctor's hypotheses preserved: ${extractedData.doctorNotes.clinicalHypotheses.length} hypotheses`);
    }

    // Determine consultation type
    const isReferral = extractedData.referralInfo?.isReferral === true;
    const consultationType = isReferral ? 'specialist_referral' : 'standard';

    return {
      patientInfo: extractedData.patientInfo || {},
      clinicalData: extractedData.clinicalData || {},
      aiQuestions: extractedData.aiQuestions || {},
      doctorNotes: extractedData.doctorNotes || {}, // ‚öïÔ∏è NOUVEAU: Hypoth√®ses du m√©decin
      referralInfo: isReferral ? extractedData.referralInfo : undefined,
      consultationType,
    };
  } catch (error: any) {
    console.error('‚ùå Extraction failed:', error.message);
    throw new Error(`Data extraction failed: ${error.message}`);
  }
}

// ============================================
// MAIN API ENDPOINT
// ============================================
export async function POST(request: NextRequest) {
  console.log('üé§ ========================================');
  console.log('   VOICE DICTATION TRANSCRIBE API');
  console.log('   (Transcription + Extraction ONLY)');
  console.log('========================================');

  try {
    // Parse form data
    const formData = await request.formData();
    const audioFile = formData.get('audioFile') as File;
    const doctorInfo = JSON.parse(formData.get('doctorInfo') as string || '{}');
    const patientId = formData.get('patientId') as string | null;

    // Validate audio file
    if (!audioFile) {
      console.error('‚ùå No audio file provided');
      return NextResponse.json(
        { success: false, error: 'No audio file provided' },
        { status: 400 }
      );
    }

    console.log('‚úÖ Request validated');
    console.log(`   Audio file: ${audioFile.name} (${audioFile.size} bytes)`);
    console.log(`   Patient ID: ${patientId || 'Not provided'}`);

    const startTime = Date.now();

    // STEP 1: Transcribe audio
    let transcription;
    try {
      console.log('\nüìù STEP 1/2: Audio Transcription');
      transcription = await transcribeAudio(audioFile);
    } catch (error: any) {
      console.error('‚ùå STEP 1 FAILED:', error.message);
      return NextResponse.json(
        {
          success: false,
          error: 'Transcription failed',
          details: error.message,
          failedAt: 'step1_transcription',
        },
        { status: 500 }
      );
    }

    // STEP 2: Extract clinical data
    let extractedData;
    try {
      console.log('\nüìù STEP 2/2: Clinical Data Extraction');
      extractedData = await extractClinicalData(transcription.text);
    } catch (error: any) {
      console.error('‚ùå STEP 2 FAILED:', error.message);
      return NextResponse.json(
        {
          success: false,
          error: 'Data extraction failed',
          details: error.message,
          failedAt: 'step2_extraction',
          transcription, // Return transcription for debugging
        },
        { status: 500 }
      );
    }

    const totalTime = Date.now() - startTime;

    console.log('\n‚úÖ ========================================');
    console.log('   TRANSCRIPTION + EXTRACTION COMPLETE');
    console.log(`   Total time: ${totalTime}ms`);
    console.log('========================================\n');

    // Return data for frontend to display and continue workflow
    return NextResponse.json({
      success: true,
      transcription: {
        text: transcription.text,
        duration: transcription.duration,
        language: transcription.language,
      },
      extractedData: {
        patientInfo: extractedData.patientInfo,
        clinicalData: extractedData.clinicalData,
        aiQuestions: extractedData.aiQuestions,
        doctorNotes: extractedData.doctorNotes, // ‚öïÔ∏è IMPORTANT: Hypoth√®ses du m√©decin
        referralInfo: extractedData.referralInfo,
        consultationType: extractedData.consultationType,
      },
      metadata: {
        processingTime: totalTime,
        audioFileName: audioFile.name,
        audioFileSize: audioFile.size,
      },
    });
  } catch (error: any) {
    console.error('‚ùå ========================================');
    console.error('   UNEXPECTED ERROR');
    console.error('========================================');
    console.error('Error:', error.message);
    console.error('Stack:', error.stack);

    return NextResponse.json(
      {
        success: false,
        error: 'An unexpected error occurred',
        details: error.message,
        stack: process.env.NODE_ENV === 'development' ? error.stack : undefined,
      },
      { status: 500 }
    );
  }
}

// Health check endpoint
export async function GET() {
  return NextResponse.json({
    status: 'OK',
    endpoint: '/api/voice-dictation-transcribe',
    description: 'Voice dictation transcription and extraction (Steps 1-2 only)',
    steps: [
      '1. Audio transcription (Whisper API)',
      '2. Clinical data extraction (GPT-4o)',
    ],
    note: 'Frontend will then display DiagnosisForm and ProfessionalReport',
  });
}
